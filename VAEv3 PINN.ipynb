{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1bc4155d",
      "metadata": {},
      "source": [
        "Perhaps add data streaming?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "54bc77d2-8b45-43e2-aaba-fa4583d44d0f",
      "metadata": {
        "id": "54bc77d2-8b45-43e2-aaba-fa4583d44d0f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "956dcabe-dbd5-4367-be08-9aa9e76d7ff1",
      "metadata": {
        "id": "956dcabe-dbd5-4367-be08-9aa9e76d7ff1"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "448caf4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time as timer\n",
        "import pickle\n",
        "\n",
        "Input_File = 'ChangeDataset622v2.pickle'\n",
        "\n",
        "def ReadPickle(filename: str) -> dict:\n",
        "  '''Reads in data from given pickle files, outputs a dictionary'''\n",
        "  try:\n",
        "    Data = pd.read_pickle(filename)\n",
        "  except FileNotFoundError:\n",
        "    raise FileNotFoundError(f'Error reading {filename}')\n",
        "  return Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "77e43952",
      "metadata": {},
      "outputs": [],
      "source": [
        "rmin = 1e-6\n",
        "rmax = 1.1\n",
        "nr = 50\n",
        "r, dr = np.linspace(rmin, rmax, nr, retstep = True)\n",
        "\n",
        "#Time Grid\n",
        "tmin = 2.9\n",
        "tmax = 4.0\n",
        "nt = 1000\n",
        "time, dt = np.linspace(tmin, tmax, nt, retstep = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3d17ceb6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Dataset -- 'ChangeDataset622v2.pickle'\n",
            "Dataset took 2.56s to load\n",
            "(10000, 4, 1000, 50)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loading Dataset -- '{Input_File}'\")\n",
        "t_Load1a = timer.time()\n",
        "Data = ReadPickle(Input_File)\n",
        "t_Load2a = timer.time()\n",
        "print(f\"Dataset took {round(t_Load2a-t_Load1a,2)}s to load\")\n",
        "\n",
        "D_data = np.array([np.tile(sample['Diffusion'],(1000,1)) for sample in Data])\n",
        "V_data = np.array([np.tile(sample['Convection'],(1000,1)) for sample in Data])\n",
        "#R_data = np.array([np.tile(sample['Rho'],(1000,1)) for sample in Data])\n",
        "#T_data = np.array([np.tile(sample['Time'],(50, 1)).T for sample in Data])\n",
        "N_data = np.array([Sample['Density'] for Sample in Data])\n",
        "S_data = np.array([Sample['Source'].T for Sample in Data])\n",
        "\n",
        "Database = np.array([np.array([D_data[i],V_data[i],N_data[i],S_data[i]]) for i in range(len(Data))])\n",
        "print(Database.shape)\n",
        "del Data\n",
        "del D_data, V_data, N_data, S_data, #R_data, T_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e0e1f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# data3 = ReadPickle('ChangeDatasetPre622v3b.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "232518b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(data3[0].keys())\n",
        "# print(data3[0]['D'])\n",
        "\n",
        "\n",
        "# data4 = []\n",
        "# for i in range(len(data3)):\n",
        "#     dataa = {}\n",
        "#     dataa['Diffusion']= data3[i]['D']\n",
        "#     dataa['Convection'] = data3[i]['V']\n",
        "#     dataa['Rho'] = r\n",
        "#     dataa['Time'] = time\n",
        "#     dataa['Density'] = data3[i]['N']\n",
        "#     dataa['Source'] = np.outer(data3[i]['ST'], data3[i]['SR'])\n",
        "#     data4.append(dataa)\n",
        "# data4 = np.array(data4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "53e7367b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def WritePickle(data_out: object, filename: str) -> None:\n",
        "  '''Writes data to pickle file'''\n",
        "  if not filename.endswith('.pickle'):\n",
        "    filename += '.pickle'\n",
        "  with open(filename, 'wb+') as f:\n",
        "    # Pickle the 'data' dictionary using the highest protocol available.\n",
        "    pickle.dump(data_out, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# WritePickle(data4, 'ChangeDataset622v3.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee58b99",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original shape: (10000, 4, 1000, 50)\n",
            "Channel 0: mean = 0.9100, std = 0.5149\n",
            "Channel 1: mean = 0.1383, std = 0.9820\n",
            "Channel 2: mean = 37906458524158672896.0000, std = 26834145827980509184.0000\n",
            "Channel 3: mean = 7390494301068411904.0000, std = 16673122821864118272.0000\n",
            "Channel 0 after standardization: mean = 0.0000, std = 1.0000\n"
          ]
        }
      ],
      "source": [
        "#This i will code myself later, had gpt code up a basic scaler to use for testing now\n",
        "\n",
        "#############################GPT CODE FOR SCALING##############################\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Assume Database is loaded with shape (num_samples, 4, 1000, 50)\n",
        "print(\"Original shape:\", Database.shape)\n",
        "\n",
        "# Prepare an empty array to hold the standardized data\n",
        "Database_channel_standardized = np.empty_like(Database)\n",
        "\n",
        "# Loop over each channel to standardize it independently.\n",
        "# Here axis=(0, 2, 3) computes statistics across all samples and the 2D dimensions for the given channel.\n",
        "for c in range(Database.shape[1]):\n",
        "    channel_data = Database[:, c, :, :]\n",
        "    channel_mean = np.mean(channel_data)\n",
        "    channel_std = np.std(channel_data)\n",
        "    print(f\"Channel {c}: mean = {channel_mean:.4f}, std = {channel_std:.4f}\")\n",
        "    \n",
        "    Database_channel_standardized[:, c, :, :] = (channel_data - channel_mean) / channel_std\n",
        "# Optionally, verify the results for one channel:\n",
        "print(\"Channel 0 after standardization: mean = {:.4f}, std = {:.4f}\".format(\n",
        "    np.mean(Database_channel_standardized[:, 3, :, :]),\n",
        "    np.std(Database_channel_standardized[:, 3, :, :])\n",
        "))\n",
        "#############################GPT CODE FOR SCALING##############################\n",
        "del Database\n",
        "del channel_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "912d2d4b-ed48-4d92-90ac-9e55f6b72198",
      "metadata": {
        "id": "912d2d4b-ed48-4d92-90ac-9e55f6b72198"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Convolutional Variational Autoencoder:\n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim=640):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        \n",
        "        # Encoder:\n",
        "        self.encoder_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=2, padding=1),# -> (8, 500, 25)\n",
        "            nn.SELU(),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),# -> (16, 250, 13)\n",
        "            nn.SELU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),# -> (32, 125, 7)\n",
        "            nn.SELU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),# -> (64, 63, 4)\n",
        "            nn.SELU()\n",
        "        )\n",
        "        \n",
        "        #Flattened conv feature size is 64*63*4 = 16,128\n",
        "        self.fc1 = nn.Linear(64*63*4,128)\n",
        "        self.fc_mu = nn.Linear(128, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
        "        \n",
        "        #Decoder:\n",
        "        self.decoder_fc = nn.Linear(latent_dim, 64*63*4)\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=0),# -> (32, 125, 7)\n",
        "            nn.SELU(),\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=(1, 0)),# -> (16, 250, 13)\n",
        "            nn.SELU(),\n",
        "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1, output_padding=(1, 0)),# -> (8, 500, 25)\n",
        "            nn.SELU(),\n",
        "            nn.ConvTranspose2d(8, 4, kernel_size=3, stride=2, padding=1, output_padding=(1, 1))# -> (4, 1000, 50)\n",
        "        )\n",
        "    \n",
        "    def encode(self, x):\n",
        "        x = self.encoder_conv(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        h = F.relu(self.fc1(x))\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "    \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    \n",
        "    def decode(self, z):\n",
        "        x = self.decoder_fc(z)\n",
        "        x = x.view(-1,64,63,4)\n",
        "        x = self.decoder_conv(x)\n",
        "        return x\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        decoded = self.decode(z)\n",
        "        return decoded, mu, logvar\n",
        "\n",
        "#Updated Loss Function using Mean Squared Error (MSE)\n",
        "def vae_loss(decoded, original, mu, logvar, r, dt=1.0, dr=1.0, lambda_pinn=1e-3):\n",
        "\n",
        "    #Unpack the decoded output\n",
        "    D_hat = decoded[:,0:1]   #Diffusion\n",
        "    v_hat = decoded[:,1:2]   #Convection\n",
        "    N_hat = decoded[:,2:3]   #Density\n",
        "    S_hat = decoded[:,3:4]   #Source\n",
        "\n",
        "    #Find datatype and device\n",
        "    dtype, device = N_hat.dtype, N_hat.device\n",
        "    \n",
        "    # tandard VAE loss\n",
        "    N_true = original[:,2:3]\n",
        "    MSE = F.mse_loss(N_hat, N_true, reduction='mean')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    #Collapse 2d transport arrays\n",
        "    D1 = D_hat[:,:,0,:].unsqueeze(2)\n",
        "    v1 = v_hat[:,:,0,:].unsqueeze(2)\n",
        "\n",
        "    #Put rho array into proper form\n",
        "    r = torch.tensor(r, dtype=torch.float32, device=device)\n",
        "    r_b = r.view(1,1,1,-1).to(N_hat.device)\n",
        "\n",
        "    #Finite Difference Kernels\n",
        "    kernel_dt = torch.tensor(\n",
        "        [[[[ -1/(2*dt) ], [ 0.], [ 1/(2*dt) ]]]],\n",
        "        device=N_hat.device,\n",
        "        dtype=dtype\n",
        "    )  # shape (1,1,3,1)\n",
        "    kernel_dr = torch.tensor(\n",
        "        [[[[ -1/(2*dr), 0.,  1/(2*dr) ]]]],\n",
        "        device=N_hat.device,\n",
        "        dtype = dtype\n",
        "    )  # shape (1,1,1,3)\n",
        "\n",
        "    #dndt with Neumann BC\n",
        "    x = F.pad(N_hat, (0,0, 1,1), mode='replicate')\n",
        "    dndt = F.conv2d(x, kernel_dt)\n",
        "\n",
        "    #Advective Term (1/r)*d_r(rvn)\n",
        "    f_adv = r_b*v1*N_hat\n",
        "    x = F.pad(f_adv, (1,1, 0,0), mode='replicate')  # pad radial dim\n",
        "    dfr = F.conv2d(x, kernel_dr)\n",
        "    adv = dfr/r_b\n",
        "\n",
        "    #Diffusive Term: (1/r)*d_r(rD*d_r(n))\n",
        "    x      = F.pad(N_hat, (1,1, 0,0), mode='replicate')\n",
        "    dn_dr  = F.conv2d(x, kernel_dr)\n",
        "    f_diff = r_b * D1 * dn_dr\n",
        "    x      = F.pad(f_diff, (1,1, 0,0), mode='replicate')\n",
        "    dfd    = F.conv2d(x, kernel_dr)\n",
        "    diff   = dfd / r_b\n",
        "\n",
        "    #Residual using Source term\n",
        "    Resid = dndt + adv - diff - S_hat\n",
        "    PINN = torch.mean(Resid**2)\n",
        "\n",
        "    return MSE + KLD + lambda_pinn*PINN, PINN, MSE, KLD\n",
        "\n",
        "#Custom Dataset to handle the numerical array\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.data.size(0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "0e2800fb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100]: Average Loss: 153.7502                                             \n",
            "Epoch [1/100]: MSE: 0.6875, KLD: 0.0003, PINN: 22.7467\n",
            "\n",
            "Epoch [2/100]: Average Loss: 0.9378                                             \n",
            "Epoch [2/100]: MSE: 1.2300, KLD: 0.0007, PINN: 43.1122\n",
            "\n",
            "Epoch [3/100]: Average Loss: 2.7809                                             \n",
            "Epoch [3/100]: MSE: 0.7421, KLD: 0.0004, PINN: 31.6220\n",
            "\n",
            "Epoch [4/100]: Average Loss: 0.8636                                             \n",
            "Epoch [4/100]: MSE: 0.7187, KLD: 0.0001, PINN: 30.7261\n",
            "\n",
            "Epoch [5/100]: Average Loss: 0.8137                                             \n",
            "Epoch [5/100]: MSE: 0.9512, KLD: 0.0001, PINN: 19.0208\n",
            "\n",
            "Epoch [6/100]: Average Loss: 0.8303                                             \n",
            "Epoch [6/100]: MSE: 0.5510, KLD: 0.0001, PINN: 16.9391\n",
            "\n",
            "Epoch [7/100]: Average Loss: 0.7870                                             \n",
            "Epoch [7/100]: MSE: 0.9015, KLD: 0.0001, PINN: 24.8232\n",
            "\n",
            "Epoch [8/100]: Average Loss: 0.7862                                             \n",
            "Epoch [8/100]: MSE: 0.5566, KLD: 0.0004, PINN: 19.8574\n",
            "\n",
            "Epoch [9/100]: Average Loss: 0.8065                                             \n",
            "Epoch [9/100]: MSE: 0.8382, KLD: 0.0001, PINN: 16.9698\n",
            "\n",
            "Epoch [10/100]: Average Loss: 0.7816                                             \n",
            "Epoch [10/100]: MSE: 0.4955, KLD: 0.0010, PINN: 41.2930\n",
            "\n",
            "Epoch [11/100]: Average Loss: 0.7848                                             \n",
            "Epoch [11/100]: MSE: 0.5316, KLD: 0.0001, PINN: 28.8757\n",
            "\n",
            "Epoch [12/100]: Average Loss: 0.7815                                             \n",
            "Epoch [12/100]: MSE: 0.5745, KLD: 0.0001, PINN: 57.1667\n",
            "\n",
            "Epoch [13/100]: Average Loss: 0.7783                                             \n",
            "Epoch [13/100]: MSE: 0.6321, KLD: 0.0057, PINN: 42.9321\n",
            "\n",
            "Epoch [14/100]: Average Loss: 0.7752                                             \n",
            "Epoch [14/100]: MSE: 0.5131, KLD: 0.0001, PINN: 19.6760\n",
            "\n",
            "Epoch [15/100]: Average Loss: 0.7727                                             \n",
            "Epoch [15/100]: MSE: 0.8106, KLD: 0.0002, PINN: 78.6442\n",
            "\n",
            "Epoch [16/100]: Average Loss: 0.7798                                             \n",
            "Epoch [16/100]: MSE: 0.2967, KLD: 0.0002, PINN: 33.3970\n",
            "\n",
            "Epoch [17/100]: Average Loss: 0.7609                                             \n",
            "Epoch [17/100]: MSE: 0.8485, KLD: 0.0001, PINN: 12.1381\n",
            "\n",
            "Epoch [18/100]: Average Loss: 0.7565                                             \n",
            "Epoch [18/100]: MSE: 0.5176, KLD: 0.0006, PINN: 72.4390\n",
            "\n",
            "Epoch [19/100]: Average Loss: 0.7473                                             \n",
            "Epoch [19/100]: MSE: 0.5426, KLD: 0.0005, PINN: 21.4659\n",
            "\n",
            "Epoch [20/100]: Average Loss: 0.7618                                             \n",
            "Epoch [20/100]: MSE: 0.8042, KLD: 0.0002, PINN: 8.5557\n",
            "\n",
            "Epoch [21/100]: Average Loss: 0.7470                                             \n",
            "Epoch [21/100]: MSE: 0.6525, KLD: 0.0018, PINN: 28.4798\n",
            "\n",
            "Epoch [22/100]: Average Loss: 0.7353                                             \n",
            "Epoch [22/100]: MSE: 0.3019, KLD: 0.0003, PINN: 8.0960\n",
            "\n",
            "Epoch [23/100]: Average Loss: 0.7503                                             \n",
            "Epoch [23/100]: MSE: 0.7393, KLD: 0.0003, PINN: 32.8233\n",
            "\n",
            "Epoch [24/100]: Average Loss: 0.7348                                             \n",
            "Epoch [24/100]: MSE: 0.8260, KLD: 0.0003, PINN: 9.4220\n",
            "\n",
            "Epoch [25/100]: Average Loss: 0.7512                                             \n",
            "Epoch [25/100]: MSE: 0.7045, KLD: 0.0021, PINN: 25.8735\n",
            "\n",
            "Epoch [26/100]: Average Loss: 0.7308                                             \n",
            "Epoch [26/100]: MSE: 0.4689, KLD: 0.0041, PINN: 49.5676\n",
            "\n",
            "Epoch [27/100]: Average Loss: 0.7432                                             \n",
            "Epoch [27/100]: MSE: 0.6831, KLD: 0.0001, PINN: 11.5807\n",
            "\n",
            "Epoch [28/100]: Average Loss: 0.7249                                             \n",
            "Epoch [28/100]: MSE: 0.5939, KLD: 0.0001, PINN: 10.3034\n",
            "\n",
            "Epoch [29/100]: Average Loss: 0.7335                                             \n",
            "Epoch [29/100]: MSE: 0.4749, KLD: 0.0012, PINN: 38.7137\n",
            "\n",
            "Epoch [30/100]: Average Loss: 0.7339                                             \n",
            "Epoch [30/100]: MSE: 0.5596, KLD: 0.0003, PINN: 117.4489\n",
            "\n",
            "Epoch [31/100]: Average Loss: 0.7359                                             \n",
            "Epoch [31/100]: MSE: 0.4849, KLD: 0.0011, PINN: 35.1780\n",
            "\n",
            "Epoch [32/100]: Average Loss: 0.7394                                             \n",
            "Epoch [32/100]: MSE: 0.4092, KLD: 0.0006, PINN: 53.8798\n",
            "\n",
            "Epoch [33/100]: Average Loss: 0.7276                                             \n",
            "Epoch [33/100]: MSE: 0.8002, KLD: 0.0001, PINN: 23.7228\n",
            "\n",
            "Epoch [34/100]: Average Loss: 0.7185                                             \n",
            "Epoch [34/100]: MSE: 0.4036, KLD: 0.0001, PINN: 13.5487\n",
            "\n",
            "Epoch [35/100]: Average Loss: 0.7281                                             \n",
            "Epoch [35/100]: MSE: 0.4370, KLD: 0.0001, PINN: 9.6687\n",
            "\n",
            "Epoch [36/100]: Average Loss: 0.7198                                             \n",
            "Epoch [36/100]: MSE: 0.3468, KLD: 0.0005, PINN: 45.0674\n",
            "\n",
            "Epoch [37/100]: Average Loss: 0.7286                                             \n",
            "Epoch [37/100]: MSE: 0.9928, KLD: 0.0002, PINN: 7.7315\n",
            "\n",
            "Epoch [38/100]: Average Loss: 0.7196                                             \n",
            "Epoch [38/100]: MSE: 0.6112, KLD: 0.0001, PINN: 83.1179\n",
            "\n",
            "Epoch [39/100]: Average Loss: 0.7340                                             \n",
            "Epoch [39/100]: MSE: 0.6486, KLD: 0.0005, PINN: 83.0180\n",
            "\n",
            "Epoch [40/100]: Average Loss: 0.7267                                             \n",
            "Epoch [40/100]: MSE: 0.4222, KLD: 0.0011, PINN: 32.9408\n",
            "\n",
            "Epoch [41/100]: Average Loss: 0.7225                                             \n",
            "Epoch [41/100]: MSE: 0.3890, KLD: 0.0001, PINN: 16.8429\n",
            "\n",
            "Epoch [42/100]: Average Loss: 0.7195                                             \n",
            "Epoch [42/100]: MSE: 1.1956, KLD: 0.0020, PINN: 35.9785\n",
            "\n",
            "Epoch [43/100]: Average Loss: 0.7203                                             \n",
            "Epoch [43/100]: MSE: 0.7428, KLD: 0.0013, PINN: 7.8908\n",
            "\n",
            "Epoch [44/100]: Average Loss: 0.7384                                             \n",
            "Epoch [44/100]: MSE: 0.5708, KLD: 0.0016, PINN: 257.7293\n",
            "\n",
            "Epoch [45/100]: Average Loss: 0.7248                                             \n",
            "Epoch [45/100]: MSE: 0.9784, KLD: 0.0004, PINN: 15.4467\n",
            "\n",
            "Epoch [46/100]: Average Loss: 0.7170                                             \n",
            "Epoch [46/100]: MSE: 0.6206, KLD: 0.0000, PINN: 16.5441\n",
            "\n",
            "Epoch [47/100]: Average Loss: 0.7184                                             \n",
            "Epoch [47/100]: MSE: 0.2814, KLD: 0.0001, PINN: 11.6692\n",
            "\n",
            "Epoch [48/100]: Average Loss: 0.7106                                             \n",
            "Epoch [48/100]: MSE: 0.8354, KLD: 0.0001, PINN: 15.7566\n",
            "\n",
            "Epoch [49/100]: Average Loss: 0.7118                                             \n",
            "Epoch [49/100]: MSE: 0.5664, KLD: 0.0001, PINN: 4.7041\n",
            "\n",
            "Epoch [50/100]: Average Loss: 0.7357                                             \n",
            "Epoch [50/100]: MSE: 0.6935, KLD: 0.0018, PINN: 4.6116\n",
            "\n",
            "Epoch [51/100]: Average Loss: 0.7400                                             \n",
            "Epoch [51/100]: MSE: 0.4981, KLD: 0.0003, PINN: 11.8319\n",
            "\n",
            "Epoch [52/100]: Average Loss: 0.7219                                             \n",
            "Epoch [52/100]: MSE: 0.9416, KLD: 0.0051, PINN: 13.5155\n",
            "\n",
            "Epoch [53/100]: Average Loss: 0.7349                                             \n",
            "Epoch [53/100]: MSE: 0.8707, KLD: 0.0015, PINN: 103.5907\n",
            "\n",
            "Epoch [54/100]: Average Loss: 0.7096                                             \n",
            "Epoch [54/100]: MSE: 0.4559, KLD: 0.0002, PINN: 101.1711\n",
            "\n",
            "Epoch [55/100]: Average Loss: 0.7124                                             \n",
            "Epoch [55/100]: MSE: 0.4206, KLD: 0.0001, PINN: 3.2646\n",
            "\n",
            "Epoch [56/100]: Average Loss: 0.6944                                             \n",
            "Epoch [56/100]: MSE: 0.2649, KLD: 0.0000, PINN: 2.8832\n",
            "\n",
            "Epoch [57/100]: Average Loss: 0.6878                                             \n",
            "Epoch [57/100]: MSE: 0.3880, KLD: 0.0000, PINN: 3.0870\n",
            "\n",
            "Epoch [58/100]: Average Loss: 0.7115                                             \n",
            "Epoch [58/100]: MSE: 0.6071, KLD: 0.0001, PINN: 13.4120\n",
            "\n",
            "Epoch [59/100]: Average Loss: 0.7107                                             \n",
            "Epoch [59/100]: MSE: 0.8831, KLD: 0.0001, PINN: 5.9171\n",
            "\n",
            "Epoch [60/100]: Average Loss: 0.7244                                             \n",
            "Epoch [60/100]: MSE: 0.8066, KLD: 0.0017, PINN: 21.0197\n",
            "\n",
            "Epoch [61/100]: Average Loss: 0.7127                                             \n",
            "Epoch [61/100]: MSE: 0.9918, KLD: 0.0008, PINN: 132.1900\n",
            "\n",
            "Epoch [62/100]: Average Loss: 0.7489                                             \n",
            "Epoch [62/100]: MSE: 0.4881, KLD: 0.0074, PINN: 77.7174\n",
            "\n",
            "Epoch [63/100]: Average Loss: 0.7165                                             \n",
            "Epoch [63/100]: MSE: 0.4280, KLD: 0.0001, PINN: 6.0108\n",
            "\n",
            "Epoch [64/100]: Average Loss: 0.6997                                             \n",
            "Epoch [64/100]: MSE: 0.3606, KLD: 0.0003, PINN: 9.1787\n",
            "\n",
            "Epoch [65/100]: Average Loss: 0.7436                                             \n",
            "Epoch [65/100]: MSE: 0.6867, KLD: 0.0002, PINN: 9.4317\n",
            "\n",
            "Epoch [66/100]: Average Loss: 0.7053                                             \n",
            "Epoch [66/100]: MSE: 0.5526, KLD: 0.0011, PINN: 9.8834\n",
            "\n",
            "Epoch [67/100]: Average Loss: 0.7123                                             \n",
            "Epoch [67/100]: MSE: 0.7732, KLD: 0.0000, PINN: 4.1124\n",
            "\n",
            "Epoch [68/100]: Average Loss: 0.6975                                             \n",
            "Epoch [68/100]: MSE: 0.3501, KLD: 0.0001, PINN: 89.5690\n",
            "\n",
            "Epoch [69/100]: Average Loss: 0.7035                                             \n",
            "Epoch [69/100]: MSE: 0.3680, KLD: 0.0001, PINN: 19.5554\n",
            "\n",
            "Epoch [70/100]: Average Loss: 0.7340                                             \n",
            "Epoch [70/100]: MSE: 1.3994, KLD: 0.0054, PINN: 7.8812\n",
            "\n",
            "Epoch [71/100]: Average Loss: 0.7083                                             \n",
            "Epoch [71/100]: MSE: 0.5943, KLD: 0.0001, PINN: 2.2810\n",
            "\n",
            "Epoch [72/100]: Average Loss: 0.6856                                             \n",
            "Epoch [72/100]: MSE: 0.6706, KLD: 0.0000, PINN: 3.1067\n",
            "\n",
            "Epoch [73/100]: Average Loss: 0.6857                                             \n",
            "Epoch [73/100]: MSE: 0.3478, KLD: 0.0001, PINN: 271.8043\n",
            "\n",
            "Epoch [74/100]: Average Loss: 0.7559                                             \n",
            "Epoch [74/100]: MSE: 1.1065, KLD: 0.0001, PINN: 8.3690\n",
            "\n",
            "Epoch [75/100]: Average Loss: 0.6894                                             \n",
            "Epoch [75/100]: MSE: 0.9366, KLD: 0.0001, PINN: 12.6830\n",
            "\n",
            "Epoch [76/100]: Average Loss: 0.8180                                             \n",
            "Epoch [76/100]: MSE: 0.9978, KLD: 0.0164, PINN: 44.7990\n",
            "\n",
            "Epoch [77/100]: Average Loss: 0.7464                                             \n",
            "Epoch [77/100]: MSE: 0.6473, KLD: 0.0002, PINN: 8.3229\n",
            "\n",
            "Epoch [78/100]: Average Loss: 0.7207                                             \n",
            "Epoch [78/100]: MSE: 0.6313, KLD: 0.0006, PINN: 15.4607\n",
            "\n",
            "Epoch [79/100]: Average Loss: 0.7411                                             \n",
            "Epoch [79/100]: MSE: 1.0349, KLD: 0.0002, PINN: 12.7090\n",
            "\n",
            "Epoch [80/100]: Average Loss: 0.7158                                             \n",
            "Epoch [80/100]: MSE: 0.7571, KLD: 0.0001, PINN: 8.2735\n",
            "\n",
            "Epoch [81/100]: Average Loss: 0.6907                                             \n",
            "Epoch [81/100]: MSE: 0.6064, KLD: 0.0000, PINN: 4.9637\n",
            "\n",
            "Epoch [82/100]: Average Loss: 0.7157                                             \n",
            "Epoch [82/100]: MSE: 1.0859, KLD: 0.1798, PINN: 474.4409\n",
            "\n",
            "Epoch [83/100]: Average Loss: 0.7842                                             \n",
            "Epoch [83/100]: MSE: 0.7819, KLD: 0.0001, PINN: 7.9929\n",
            "\n",
            "Epoch [84/100]: Average Loss: 0.7806                                             \n",
            "Epoch [84/100]: MSE: 1.0252, KLD: 0.0001, PINN: 12.2580\n",
            "\n",
            "Epoch [85/100]: Average Loss: 0.7259                                             \n",
            "Epoch [85/100]: MSE: 0.6607, KLD: 0.0005, PINN: 17.8832\n",
            "\n",
            "Epoch [86/100]: Average Loss: 0.7320                                             \n",
            "Epoch [86/100]: MSE: 0.3088, KLD: 0.0001, PINN: 17.0357\n",
            "\n",
            "Epoch [87/100]: Average Loss: 0.7198                                             \n",
            "Epoch [87/100]: MSE: 0.7237, KLD: 0.0001, PINN: 212.7288\n",
            "\n",
            "Epoch [88/100]: Average Loss: 0.7251                                             \n",
            "Epoch [88/100]: MSE: 0.7527, KLD: 0.0001, PINN: 22.5873\n",
            "\n",
            "Epoch [89/100]: Average Loss: 0.6899                                             \n",
            "Epoch [89/100]: MSE: 0.7366, KLD: 0.0000, PINN: 4.8399\n",
            "\n",
            "Epoch [90/100]: Average Loss: 0.6855                                             \n",
            "Epoch [90/100]: MSE: 0.8535, KLD: 0.0000, PINN: 3.4210\n",
            "\n",
            "Epoch [91/100]: Average Loss: 0.6842                                             \n",
            "Epoch [91/100]: MSE: 0.8438, KLD: 0.0001, PINN: 7.7824\n",
            "\n",
            "Epoch [92/100]: Average Loss: 0.7137                                             \n",
            "Epoch [92/100]: MSE: 0.9251, KLD: 0.0001, PINN: 7.6997\n",
            "\n",
            "Epoch [93/100]: Average Loss: 0.7023                                             \n",
            "Epoch [93/100]: MSE: 0.3407, KLD: 0.0002, PINN: 69.9397\n",
            "\n",
            "Epoch [94/100]: Average Loss: 0.7467                                             \n",
            "Epoch [94/100]: MSE: 0.3802, KLD: 0.0001, PINN: 3.2724\n",
            "\n",
            "Epoch [95/100]: Average Loss: 0.8730                                             \n",
            "Epoch [95/100]: MSE: 1.0095, KLD: 0.0005, PINN: 44.1234\n",
            "\n",
            "Epoch [96/100]: Average Loss: 0.7241                                             \n",
            "Epoch [96/100]: MSE: 0.5516, KLD: 0.0005, PINN: 5.1111\n",
            "\n",
            "Epoch [97/100]: Average Loss: 0.7122                                             \n",
            "Epoch [97/100]: MSE: 0.5927, KLD: 0.0001, PINN: 2.3578\n",
            "\n",
            "Epoch [98/100]: Average Loss: 0.6959                                             \n",
            "Epoch [98/100]: MSE: 0.8308, KLD: 0.0000, PINN: 3.4232\n",
            "\n",
            "Epoch [99/100]: Average Loss: 0.6843                                             \n",
            "Epoch [99/100]: MSE: 0.6227, KLD: 0.0000, PINN: 2.7274\n",
            "\n",
            "Epoch [100/100]: Average Loss: 0.7818                                             \n",
            "Epoch [100/100]: MSE: 0.7664, KLD: 0.0000, PINN: 9.2464\n",
            "\n",
            "Training complete!                                                                      \n"
          ]
        }
      ],
      "source": [
        "#Training Setup\n",
        "latent_dim = 640\n",
        "batch_size = 32\n",
        "lr_vae = 1e-3  #1e-4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vae = VariationalAutoencoder(latent_dim=640).to(device)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=lr_vae)\n",
        "\n",
        "#Dataloaders\n",
        "dataset = CustomDataset(Database_channel_standardized)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#Training Loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    total_samples = 0\n",
        "    epoch_loss = 0\n",
        "    epoch_loss_mse = 0\n",
        "    epoch_loss_kld = 0\n",
        "    epoch_loss_pinn = 0\n",
        "    \n",
        "    for batch_data, _ in dataloader:\n",
        "        bs = batch_data.size(0)\n",
        "        batch_data = batch_data.to(device)\n",
        "        decoded_data, mu, logvar = vae(batch_data)\n",
        "        \n",
        "        loss, pinn, mse, kld = vae_loss(decoded_data, batch_data, mu, logvar, r, dt, dr, lambda_pinn=1e-3)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()*bs\n",
        "        total_samples += bs\n",
        "    \n",
        "    avg_loss = epoch_loss / total_samples\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]: Average Loss: {avg_loss:.4f}\" + ' '*45)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}]: MSE: {mse:.4f}, KLD: {kld:.4f}, PINN: {pinn:.4f}\\n')\n",
        "    \n",
        "    prog_str = (f\"Epoch [{epoch+1}/{num_epochs}]: \"\n",
        "                f\"Avg: {avg_loss:.4f}, MSE: {mse:.4f}, KLD: {kld:.4f}, PINN: {pinn:.4f}\")\n",
        "    print(prog_str, end='\\r', flush=True)\n",
        "    \n",
        "print(\"Training complete!\" + ' '*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e303a747",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(vae, 'VAE_Pretrained_PINN_v1.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62c2e3b1",
      "metadata": {},
      "source": [
        "Add pinn residual scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZgL3Kt-knmXI",
      "metadata": {
        "id": "ZgL3Kt-knmXI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Set the VAE to evaluation mode\n",
        "vae.eval()\n",
        "\n",
        "#Specify the latent dimension (should match what you used during training)\n",
        "latent_dim = 640\n",
        "\n",
        "#Sample a single latent vector from a standard normal distribution and decode it.\n",
        "with torch.no_grad():\n",
        "    z1 = torch.randn(1, latent_dim, device=device)  #Sample one latent vector\n",
        "    z2 = torch.randn(1, latent_dim, device=device)  #Sample one latent vector\n",
        "    generated1 = vae.decode(z1)\n",
        "    generated2 = vae.decode(z2)\n",
        "\n",
        "#Remove the batch dimension and move the tensor to CPU\n",
        "sample1 = generated1.squeeze(0).cpu()\n",
        "sample2 = generated2.squeeze(0).cpu()\n",
        "sample1_np = sample1.numpy()\n",
        "sample2_np = sample2.numpy()\n",
        "\n",
        "#Plotting\n",
        "fig, axs = plt.subplots(2, 3, figsize=(10,7))\n",
        "axs = axs.ravel()\n",
        "id_num = np.random.randint(10000)\n",
        "\n",
        "\n",
        "for i in range(6):\n",
        "    im = axs[i].imshow(sample1_np[i], aspect='auto', cmap='viridis')\n",
        "    axs[i].set_title(f'Generated Channel {i}')\n",
        "    axs[i].axis('off')\n",
        "    fig.colorbar(im, ax=axs[i], fraction=0.046, pad=0.04)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig.savefig(f'GeneratedChannelsv{id_num}.png', dpi=300, bbox_inches='tight')\n",
        "    \n",
        "fig2, axs2 = plt.subplots(2, 3, figsize=(10,7))\n",
        "axs2 = axs2.ravel()\n",
        "\n",
        "for i in range(6):\n",
        "    im = axs2[i].imshow(Database_channel_standardized[np.random.randint(1000)][i], aspect='auto', cmap='viridis')\n",
        "    axs2[i].set_title(f'Base Channel {i}')\n",
        "    axs2[i].axis('off')\n",
        "    fig2.colorbar(im, ax=axs2[i], fraction=0.046, pad=0.04)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig.savefig(f'BaseChannel{id_num}.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "fig, axs = plt.subplots(2, 3, figsize=(10,7))\n",
        "axs = axs.ravel()\n",
        "\n",
        "for i in range(6):\n",
        "    im = axs[i].imshow((sample1_np[i] - sample2_np[i]), aspect='auto', cmap='viridis')\n",
        "    axs[i].set_title(f'Variance Channel {i}')\n",
        "    axs[i].axis('off')\n",
        "    fig.colorbar(im, ax=axs[i], fraction=0.046, pad=0.04)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig.savefig(f'VarianceChannel{id_num}.png', dpi=300, bbox_inches='tight')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47a4f95",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Poolng, fix noise, etc."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
